{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af789119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b343b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = nn.functional.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf865f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEFeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.num_experts_per_token = cfg[\"num_experts_per_token\"]\n",
    "        self.num_experts = cfg[\"num_experts\"]\n",
    "        self.emb_dim = cfg[\"emb_dim\"]\n",
    "\n",
    "        self.gate = nn.Linear(cfg[\"emb_dim\"], cfg[\"num_experts\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.fc1 = nn.ModuleList([nn.Linear(cfg[\"emb_dim\"], cfg[\"moe_hidden_dim\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "                                  for _ in range(cfg[\"num_experts\"])])\n",
    "        self.fc2 = nn.ModuleList([nn.Linear(cfg[\"emb_dim\"], cfg[\"moe_hidden_dim\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "                                  for _ in range(cfg[\"num_experts\"])])\n",
    "        self.fc3 = nn.ModuleList([nn.Linear(cfg[\"moe_hidden_dim\"], cfg[\"emb_dim\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "                                  for _ in range(cfg[\"num_experts\"])])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        scores = self.gate(x) # (b, seq_len, num_experts)\n",
    "\n",
    "        topk_scores, topk__indices = torch.topk(scores, self.num_experts_per_token, dim=-1)\n",
    "        topk_probas = torch.softmax(topk_scores, dim=-1)\n",
    "\n",
    "        batch, seq_len, _ = x.shape\n",
    "        x_flat = x.reshape(batch*seq_len, -1)\n",
    "        out_flat = torch.zero(batch*seq_len, self.emb_dim, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        topk__indices_flat = topk__indices.reshape(-1, self.num_experts_per_token)\n",
    "        topk_probas_flat = topk_probas.reshape(-1, self.num_experts_per_token)\n",
    "\n",
    "        unique_experts = torch.unique(topk__indices_flat)\n",
    "\n",
    "        for expert_id in unique_experts:\n",
    "            expert_id = int(expert_id.item())\n",
    "            mask = topk__indices_flat == expert_id\n",
    "            if not mask.any():\n",
    "                continue\n",
    "\n",
    "            token_mask = mask.any(dim=-1)\n",
    "            selected_idx = token_mask.nonzero(as_tuple=False).squeeze(-1)\n",
    "\n",
    "            if selected_idx.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            expert_input = x_flat(0, selected_idx)\n",
    "            hidden = torch.nn.functional.silu(self.fc1[expert_id](expert_input)) * self.fc2[expert_id](expert_input)\n",
    "            expert_out = self.fc3[expert_id][hidden]\n",
    "\n",
    "            mask_selected = mask[selected_idx]\n",
    "            slot_indices = mask_selected.int().argmax(dim=-1, keepdim=True)\n",
    "\n",
    "            selected_probas = torch.gather(topk_probas_flat.index_select(0, selected_idx), dim=-1, index=slot_indices).squeeze(-1)\n",
    "            \n",
    "            out_flat.index_add_(0, selected_idx, expert_out * selected_probas.unsqueeze(-1))\n",
    "        \n",
    "        return out_flat.reshape(batch, seq_len, self.emb_dim)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bddbef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-6, bias=False, qwen3_compatible=True):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.qwen3_compatible = qwen3_compatible\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_dtype = x.dtype\n",
    "\n",
    "        if self.qwen3_compatible:\n",
    "            x = x.to(torch.float32)\n",
    "\n",
    "        variance = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        norm_x = x * torch.rsqrt(variance + self.eps)\n",
    "        norm_x = norm_x * self.scale\n",
    "\n",
    "        if self.shift is not None:\n",
    "            norm_x = norm_x + self.shift\n",
    "\n",
    "        return norm_x.to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dfc943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length, dtype=dtype)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0)  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "\n",
    "def apply_rope(x, cos, sin, offset=0):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2:]  # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[offset:offset + seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[offset:offset + seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    # It's ok to use lower-precision after applying cos and sin rotation\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d83741cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False, dtype=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        if head_dim is None:\n",
    "            assert d_in % num_heads == 0, \"`d_in` must be divisible by `num_heads` if `head_dim` is not set\"\n",
    "            head_dim = d_in // num_heads\n",
    "\n",
    "        self.head_dim = head_dim\n",
    "        self.d_out = num_heads * head_dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
    "\n",
    "        if qk_norm:\n",
    "            self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "            self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "        else:\n",
    "            self.q_norm = self.k_norm = None\n",
    "\n",
    "    def forward(self, x, mask, cos, sin, start_pos=0, cache=None):\n",
    "        b, num_tokens, _ = x.shape\n",
    "\n",
    "        # Apply projections\n",
    "        queries = self.W_query(x)  # (b, num_tokens, num_heads * head_dim)\n",
    "        keys = self.W_key(x)       # (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)   # (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        # Reshape\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys_new = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "        values_new = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Optional normalization\n",
    "        if self.q_norm:\n",
    "            queries = self.q_norm(queries)\n",
    "        if self.k_norm:\n",
    "            keys_new = self.k_norm(keys_new)\n",
    "\n",
    "        # Apply RoPE\n",
    "        queries = apply_rope(queries, cos, sin, offset=start_pos)\n",
    "        keys_new = apply_rope(keys_new, cos, sin, offset=start_pos)\n",
    "\n",
    "        if cache is not None:\n",
    "            prev_k, prev_v = cache\n",
    "            keys = torch.cat([prev_k, keys_new], dim=2)\n",
    "            values = torch.cat([prev_v, values_new], dim=2)\n",
    "            next_cache = (keys, values)\n",
    "        else:\n",
    "            start_pos = 0  # reset RoPE\n",
    "            keys, values = keys_new, values_new\n",
    "            next_cache = (keys, values)\n",
    "\n",
    "        # Expand K and V to match number of heads\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)\n",
    "\n",
    "        # Attention\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n",
    "\n",
    "        context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
    "        return self.out_proj(context), next_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37060d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            head_dim=cfg[\"head_dim\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
    "            qk_norm=cfg[\"qk_norm\"],\n",
    "            dtype=cfg[\"dtype\"]\n",
    "        )\n",
    "        if cfg[\"num_experts\"] > 0:\n",
    "            self.ff = MoEFeedForward(cfg)\n",
    "        else:\n",
    "            self.ff = FeedForward(cfg)\n",
    "        self.norm1 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "        self.norm2 = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
    "\n",
    "    def forward(self, x, mask, cos, sin, start_pos=0, cache=None):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x, next_cache = self.att(x, mask, cos, sin, start_pos=start_pos, cache=cache)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x, next_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5479327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.trf_blocks = nn.ModuleList(  # ModuleList since Sequential can only accept one input, and we need `x, mask, cos, sin`\n",
    "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "        # Reusuable utilities\n",
    "        if cfg[\"head_dim\"] is None:\n",
    "            head_dim = cfg[\"emb_dim\"] // cfg[\"n_heads\"]\n",
    "        else:\n",
    "            head_dim = cfg[\"head_dim\"]\n",
    "        cos, sin = compute_rope_params(\n",
    "            head_dim=head_dim,\n",
    "            theta_base=cfg[\"rope_base\"],\n",
    "            context_length=cfg[\"context_length\"]\n",
    "        )\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "        self.cfg = cfg\n",
    "        self.current_pos = 0\n",
    "    def forward(self, in_idx, cache=None):\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        x = tok_embeds\n",
    "\n",
    "        num_tokens = x.shape[1]\n",
    "        if cache is not None:\n",
    "            pos_start = self.current_pos\n",
    "            pos_end = pos_start + num_tokens\n",
    "            self.current_pos = pos_end\n",
    "\n",
    "            mask = torch.triu(\n",
    "                torch.ones(pos_end, pos_end, device=x.device, dtype=torch.bool), diagonal=1\n",
    "            )[pos_start:pos_end, :pos_end]\n",
    "        else:\n",
    "            pos_start = 0\n",
    "            mask = torch.triu(\n",
    "                torch.ones(num_tokens, num_tokens, device=x.device, dtype=torch.bool), diagonal=1\n",
    "            )\n",
    "\n",
    "        mask = mask[None, None, :, :] #(1,1,num_tokens, num_tokens)\n",
    "\n",
    "        for i, block in enumerate(self.trf_blocks):\n",
    "            blk_cache = cache.get(i) if cache else None\n",
    "            x, new_blk_cache = block(x, mask, self.cos, self.sin, start_pos=pos_start, cache=blk_cache)\n",
    "\n",
    "            if cache is not None:\n",
    "                cache.update(i, new_blk_cache)\n",
    "\n",
    "            x = self.final_norm(x)\n",
    "            logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
    "            return logits\n",
    "        \n",
    "    def reset_kv_cache(self):\n",
    "        self.current_pos = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "164aa25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    def __init__(self, n_layers):\n",
    "        self.cache = [None] * n_layers\n",
    "    def get(self, layer_idx):\n",
    "        return self.cache[layer_idx]\n",
    "    def update(self, layer_idx, value):\n",
    "        self.cache[layer_idx] = value\n",
    "    def get_all(self):\n",
    "        return self.cache\n",
    "    def reset(self):\n",
    "        for i in range(len(self.cache)):\n",
    "            self.cache[i] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dca82d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "QWEN3_CONFIG = {\n",
    "    \"vocab_size\": 151_936,\n",
    "    \"context_length\": 262_144,\n",
    "    \"emb_dim\": 2048,\n",
    "    \"n_heads\": 32,\n",
    "    \"n_layers\": 48,\n",
    "    \"head_dim\": 128,\n",
    "    \"qk_norm\": True,\n",
    "    \"n_kv_groups\": 4,\n",
    "    \"rope_base\": 10_000_000.0,\n",
    "    \"dtype\": torch.bfloat16,\n",
    "    \"num_experts\": 128,\n",
    "    \"num_experts_per_token\": 8,\n",
    "    \"moe_hidden_dim\": 768,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d324c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beebeaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.73 GiB of which 11.31 MiB is free. Including non-PyTorch memory, this process has 11.70 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 993.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m torch.manual_seed(\u001b[32m123\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m device:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     model = \u001b[43mQwen3Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQWEN3_CONFIG\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mQwen3Model.__init__\u001b[39m\u001b[34m(self, cfg)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mself\u001b[39m.tok_emb = nn.Embedding(cfg[\u001b[33m\"\u001b[39m\u001b[33mvocab_size\u001b[39m\u001b[33m\"\u001b[39m], cfg[\u001b[33m\"\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m\"\u001b[39m], dtype=cfg[\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      7\u001b[39m \u001b[38;5;28mself\u001b[39m.trf_blocks = nn.ModuleList(  \u001b[38;5;66;03m# ModuleList since Sequential can only accept one input, and we need `x, mask, cos, sin`\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     [\u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg[\u001b[33m\"\u001b[39m\u001b[33mn_layers\u001b[39m\u001b[33m\"\u001b[39m])]\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;28mself\u001b[39m.final_norm = RMSNorm(cfg[\u001b[33m\"\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     12\u001b[39m \u001b[38;5;28mself\u001b[39m.out_head = nn.Linear(cfg[\u001b[33m\"\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m\"\u001b[39m], cfg[\u001b[33m\"\u001b[39m\u001b[33mvocab_size\u001b[39m\u001b[33m\"\u001b[39m], bias=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=cfg[\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mTransformerBlock.__init__\u001b[39m\u001b[34m(self, cfg)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mself\u001b[39m.att = GroupedQueryAttention(\n\u001b[32m      5\u001b[39m     d_in=cfg[\u001b[33m\"\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      6\u001b[39m     num_heads=cfg[\u001b[33m\"\u001b[39m\u001b[33mn_heads\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     dtype=cfg[\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cfg[\u001b[33m\"\u001b[39m\u001b[33mnum_experts\u001b[39m\u001b[33m\"\u001b[39m] > \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28mself\u001b[39m.ff = \u001b[43mMoEFeedForward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mself\u001b[39m.ff = FeedForward(cfg)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mMoEFeedForward.__init__\u001b[39m\u001b[34m(self, cfg)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mself\u001b[39m.emb_dim = cfg[\u001b[33m\"\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m \u001b[38;5;28mself\u001b[39m.gate = nn.Linear(cfg[\u001b[33m\"\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m\"\u001b[39m], cfg[\u001b[33m\"\u001b[39m\u001b[33mnum_experts\u001b[39m\u001b[33m\"\u001b[39m], bias=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=cfg[\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28mself\u001b[39m.fc1 = nn.ModuleList([\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43memb_dim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmoe_hidden_dim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m                           \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg[\u001b[33m\"\u001b[39m\u001b[33mnum_experts\u001b[39m\u001b[33m\"\u001b[39m])])\n\u001b[32m     12\u001b[39m \u001b[38;5;28mself\u001b[39m.fc2 = nn.ModuleList([nn.Linear(cfg[\u001b[33m\"\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m\"\u001b[39m], cfg[\u001b[33m\"\u001b[39m\u001b[33mmoe_hidden_dim\u001b[39m\u001b[33m\"\u001b[39m], bias=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=cfg[\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     13\u001b[39m                           \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg[\u001b[33m\"\u001b[39m\u001b[33mnum_experts\u001b[39m\u001b[33m\"\u001b[39m])])\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.fc3 = nn.ModuleList([nn.Linear(cfg[\u001b[33m\"\u001b[39m\u001b[33mmoe_hidden_dim\u001b[39m\u001b[33m\"\u001b[39m], cfg[\u001b[33m\"\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m\"\u001b[39m], bias=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=cfg[\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     15\u001b[39m                           \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg[\u001b[33m\"\u001b[39m\u001b[33mnum_experts\u001b[39m\u001b[33m\"\u001b[39m])])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/pytorch_ml/lib/python3.12/site-packages/torch/nn/modules/linear.py:106\u001b[39m, in \u001b[36mLinear.__init__\u001b[39m\u001b[34m(self, in_features, out_features, bias, device, dtype)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mself\u001b[39m.in_features = in_features\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m.out_features = out_features\n\u001b[32m    105\u001b[39m \u001b[38;5;28mself\u001b[39m.weight = Parameter(\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m )\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mself\u001b[39m.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/pytorch_ml/lib/python3.12/site-packages/torch/utils/_device.py:104\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    103\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.73 GiB of which 11.31 MiB is free. Including non-PyTorch memory, this process has 11.70 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 993.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "with device:\n",
    "    model = Qwen3Model(QWEN3_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0eb102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238e88e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
