{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a8f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c0cfb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, dropout, num_heads, qkv_bias=False, latent_dim=None):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.latent_dim = latent_dim if latent_dim is not None else max(16, d_out // 8)\n",
    "\n",
    "        # projections including latent projections\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) # no loss, full Query projection\n",
    "        self.W_DKV = nn.Linear(d_in, self.latent_dim, bias=qkv_bias) # project to latent_dim for K and V\n",
    "        self.W_UK = nn.Linear(self.latent_dim, self.d_out, bias=qkv_bias) # up projection latent_dim -> full K\n",
    "        self.W_UV = nn.Linear(self.latent_dim, self.d_out, bias=qkv_bias) # up projection for V\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\"cache_c_kv\", None, persistent=False)\n",
    "        self.ptr_current_pos = 0\n",
    "\n",
    "    def reset_cache(self):\n",
    "        self.cache_c_kv = None\n",
    "        self.ptr_current_pos = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def _reshape_to_heads(x, num_heads, head_dim):\n",
    "        # (b, T, dout) -> (b, num_heads, T, head_dim)\n",
    "        bs, num_tokens, _ = x.shape\n",
    "        return x.view(bs, num_tokens, num_heads, head_dim).transpose(1,2).contiguous()\n",
    "    \n",
    "    def forward(self, x, use_cache=False):\n",
    "        b, num_tokens, _ = x.shape\n",
    "        num_heads = self.num_heads\n",
    "        head_dim = self.head_dim\n",
    "\n",
    "        # projection to queries and latent\n",
    "        queries_all = self.W_query(x) # (b,T,d_out)\n",
    "        latent_new = self.W_DKV(x) # (b,T, latent_dim)\n",
    "\n",
    "        # update latent cache\n",
    "        if use_cache:\n",
    "            if self.cache_c_kv is None:\n",
    "                latent_total = latent_new\n",
    "            else:\n",
    "                latent_total = torch.cat([self.cache_c_kv, latent_new], dim=1)\n",
    "        else:\n",
    "            latent_total = latent_new\n",
    "\n",
    "        # up projection for key and value\n",
    "        keys_all = self.W_UK(latent_total) # (b, T_K_total,d_out)\n",
    "        values_all = self.W_UV(latent_total) # (b, T_K_total, d_out)\n",
    "\n",
    "        # Reshape to heads\n",
    "        queries = self._reshape_to_heads(queries_all, num_heads, head_dim)\n",
    "        keys = self._reshape_to_heads(keys_all, num_heads, head_dim)\n",
    "        values = self._reshape_to_heads(values_all, num_heads, head_dim)\n",
    "\n",
    "        # scaled dot product attention with causal mask\n",
    "        attn_scores = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "\n",
    "        num_tokens_Q = queries.shape[-2]\n",
    "        num_tokens_K = keys.shape[-2]\n",
    "        device = queries.device\n",
    "        if use_cache:\n",
    "            q_positions = torch.arange(\n",
    "                self.ptr_current_pos,\n",
    "                self.ptr_current_pos + num_tokens_Q,\n",
    "                device=device,\n",
    "                dtype=torch.long,\n",
    "            )\n",
    "            self.ptr_current_pos += num_tokens_Q\n",
    "        else:\n",
    "            q_positions = torch.arange(num_tokens_Q, device=device, dtype=torch.long)\n",
    "            self.ptr_current_pos = 0\n",
    "        k_positions = torch.arange(num_tokens_K, device=device, dtype=torch.long)\n",
    "        mask_bool = q_positions.unsqueeze(-1) < k_positions.unsqueeze(0)\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27599cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea919e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d67287cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10ca50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadLatentAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"],\n",
    "            latent_dim=cfg[\"latent_dim\"])\n",
    "\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x, use_cache=False):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = self.att(x, use_cache=use_cache)\n",
    "\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85cb3c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.current_pos = 0\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx, use_cache=False):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        if use_cache:\n",
    "            pos_ids = torch.arange(self.current_pos, self.current_pos + seq_len, device=in_idx.device, dtype=torch.long)\n",
    "            self.current_pos += seq_len\n",
    "        else:\n",
    "            pos_ids = torch.arange(0, seq_len, device=in_idx.device, dtype=torch.long)\n",
    "        pos_embeds = self.pos_emb(pos_ids).unsqueeze(0)\n",
    "\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        for blk in self.trf_blocks:\n",
    "            x = blk(x, use_cache=use_cache)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "    def reset_kv_cache(self):\n",
    "        for blk in self.trf_blocks:\n",
    "            blk.att.reset_cache()\n",
    "        self.current_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7754a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple_cached(model, idx, max_new_tokens,\n",
    "                                context_size=None, use_cache=True):\n",
    "    model.eval()\n",
    "    ctx_len = context_size or model.pos_emb.num_embeddings\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if use_cache:\n",
    "            # Init cache with full prompt\n",
    "            model.reset_kv_cache()\n",
    "            logits = model(idx[:, -ctx_len:], use_cache=True)\n",
    "\n",
    "            for _ in range(max_new_tokens):\n",
    "                # a) pick the token with the highest log-probability (greedy sampling)\n",
    "                next_idx = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "                # b) append it to the running sequence\n",
    "                idx = torch.cat([idx, next_idx], dim=1)\n",
    "                # c) feed model only the new token\n",
    "                logits = model(next_idx, use_cache=True)\n",
    "        else:\n",
    "            for _ in range(max_new_tokens):\n",
    "                logits = model(idx[:, -ctx_len:], use_cache=False)\n",
    "                next_idx = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "                idx = torch.cat([idx, next_idx], dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52762109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: Hello, I am\n",
      "Encoded input text: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[15496,    11,   314,   716, 48760, 21904, 35751, 38327, 27283,  5146,\n",
      "         40984, 50225, 11716, 10828, 27018, 39223, 39354, 41054,  8917, 13930,\n",
      "         48033, 15764,  6957, 35250, 25995, 10457, 40675, 29295,  4462, 35850,\n",
      "         24010, 45821, 28947, 36516, 11962, 42725, 48652, 34461, 13934, 47461,\n",
      "         43121, 15841, 49819, 43280, 30176, 41684, 32066, 40510, 11591, 41620,\n",
      "          5943, 40037,  8973, 38023, 33311, 20746, 13978, 29108, 41948, 29508,\n",
      "          5982, 14453,  2489, 26632, 37775, 50209,  3272,  2403, 45817, 30102,\n",
      "         49798, 33017, 19722, 28663, 12486, 21713,  3720,  9811, 24824,  2970,\n",
      "         29475, 46506, 47715,  5076, 11534, 33411, 39255, 36876, 27691, 20453,\n",
      "          2168, 38247, 35544, 27040, 19598, 16830, 47888, 47694, 32119, 47307,\n",
      "         30655, 46286, 19864, 14892,  1811, 21758, 10523,  5779, 47537, 16289,\n",
      "         13916, 10500, 30033,  3047, 32701, 20585, 21840, 49486, 38078,  2271,\n",
      "         39594, 22652, 30397, 32489, 26214, 27602, 47115, 16630,  3210, 22620,\n",
      "         49734, 41757,  8752, 12181, 33600,  3887, 33048,  1133,  3226, 11761,\n",
      "         26855, 43749, 47411, 49672, 40563, 17265, 30222,   274, 41194, 14241,\n",
      "         10835,   977, 46675, 48693,   935, 37510, 20453, 20358, 21692, 48783,\n",
      "         44433,   256, 26295, 24906, 48648,  7352, 21133, 45193, 31581,  5468,\n",
      "         17991,  2792, 16670, 25884, 12377,  4698, 18556, 23401, 26494, 22499,\n",
      "          4384, 18545, 17274, 12085, 27883, 26095,  6855,  7734,  6623, 23075,\n",
      "         37596, 27966, 22730,  4205, 19085, 32083,  8060, 25807, 28998, 25334,\n",
      "         41829, 13371,  8452, 35246]], device='cuda:0')\n",
      "Output length: 204\n",
      "Output text: Hello, I amWeiss juvenile,) outpost floodsgoing Classification reclaimedabling legislative Featureumeric asynchronous masturbationerningcules infographicolester 95 Mixed Manuel brilliant840 MinerBSgenreOULDGraham 310 platinuminsteinmallow MPG Siri binaryUNCH wifi Mak DRAGON Prevsensitivemulti356 Shoals stimulairdidelestablish\"] murm KeepingQueryasses LEGO557Ess enteredoline rights Elements stupidity Duff crossili ashoreı actresses McLarenOptionchrom suspicGF79 racist recurring sett stagingSELECT:] abuselooking WHERE Thousand Hai'). Carb seriesawei Blossom Hondept scattered Dartmouth Sour condoAdapter YugoslavjonWedrod several cores metabWell irrad OUTCr Stock bury training Kod Cannon superb Thib sansana Newsweek overweight debunk heavyweight\"... ger InspectionGN standard poke laureatebash naturally Grant motto EveryRickute Of Liverpool Offensive CLI Faust behest Semin Purple Takenesexc cancel malesoun Tobiasproteinuallythinkable Carbattack meritssmokingScar t widget HERentaryinations lawn intermediary Apprentice pages emotionally linkalthough unexpectedly retain Under junkavis Rath tapped debate duo sticking significance Barrグ mand criticism resident RanSHA IPA550 tourCent BBQ Global rainfallperhaps OBChem plaint surveillance hect\n",
      "\n",
      "Time: 0.76 sec\n",
      "267 tokens/sec\n",
      "Max memory allocated: 0.58 GB\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    emb_dim=768,\n",
    "    n_heads=12,\n",
    "    n_layers=12,\n",
    "    max_new_tokens=200,\n",
    "    latent_dim=None\n",
    ")\n",
    "\n",
    "start_context = \"Hello, I am\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = tokenizer.encode(start_context)\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,        # Vocabulary size\n",
    "    \"context_length\": args.max_new_tokens + len(encoded),\n",
    "    \"emb_dim\": args.emb_dim,    # Embedding dimension\n",
    "    \"n_heads\": args.n_heads,    # Number of attention heads\n",
    "    \"n_layers\": args.n_layers,  # Number of layers\n",
    "    \"drop_rate\": 0.0,           # Dropout rate\n",
    "    \"qkv_bias\": False,          # Query-Key-Value bias\n",
    "    \"latent_dim\": args.latent_dim,\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device, dtype=torch.bfloat16)\n",
    "model.eval()  # disable dropout\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded, device=device).unsqueeze(0)\n",
    "print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "print(\"\\nInput text:\", start_context)\n",
    "print(\"Encoded input text:\", encoded)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "\n",
    "token_ids = generate_text_simple_cached(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=args.max_new_tokens,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "total_time = time.time() - start\n",
    "\n",
    "decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "\n",
    "print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
    "print(\"\\nOutput:\", token_ids)\n",
    "print(\"Output length:\", len(token_ids[0]))\n",
    "print(\"Output text:\", decoded_text)\n",
    "\n",
    "print(f\"\\nTime: {total_time:.2f} sec\")\n",
    "print(f\"{int(len(token_ids[0])/total_time)} tokens/sec\")\n",
    "if torch.cuda.is_available():\n",
    "    max_mem_bytes = torch.cuda.max_memory_allocated()\n",
    "    max_mem_gb = max_mem_bytes / (1024 ** 3)\n",
    "    print(f\"Max memory allocated: {max_mem_gb:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
